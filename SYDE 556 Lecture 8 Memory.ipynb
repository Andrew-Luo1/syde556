{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYDE 556/750: Simulating Neurobiological Systems\n",
    "\n",
    "## Memory\n",
    "\n",
    "- We've seen how to represent symbol-like structures using vectors\n",
    "- Typically high dimensional vectors\n",
    "- How can we store those over short periods of time (working memory)\n",
    "- Long periods of time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remembering a Plan\n",
    "\n",
    "- Previously, we talked about a model of the Tower of Hanoi task\n",
    "- Move disks around from one configuration to another\n",
    "- A common situation:\n",
    "    - Trying to move disk 3 to peg A, but disk 2 is in the way, so we need to move disk 2 to peg B, but disk 1 is in the way, so we move disk 1 to peg C\n",
    "    - When we do that, it'd be nice to be able to remember the previous steps in the chain of reasoning\n",
    "    - So we could go back to trying to move disk 2 to peg B, rather than going all the way back to the beginning\n",
    "    - Timing data indicates people do this\n",
    "- What do we need to store?    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Need to remember disks and what peg they go to\n",
    "    - can't do `D3 + A  + D2 + B`\n",
    "    - since that's the same as `D3 + B  + D2 + A`\n",
    "- Something like: `D3` $\\circledast$ `A + D2` $\\circledast$ `B` \n",
    "- Associative Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's start by just computing the convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named nef",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-eb99e788dd22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnef\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Symbols'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquick\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Create the network object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named nef"
     ]
    }
   ],
   "source": [
    "D = 64\n",
    "subdim = 8\n",
    "N = 500\n",
    "\n",
    "import nef\n",
    "net=nef.Network('Symbols', fixed_seed=1, quick=True) #Create the network object\n",
    "\n",
    "net.make('A',neurons=1,dimensions=D,mode='direct')  # don't bother simulating these neurons\n",
    "net.make('B',neurons=1,dimensions=D,mode='direct')  # don't bother simulating these neurons\n",
    "\n",
    "net.make_array('C',N,D/subdim,dimensions=subdim,radius=1.0/math.sqrt(D), seed=2) \n",
    "\n",
    "conv = nef.convolution.make_convolution(net,'*','A','B','C',200, seed=3)\n",
    "\n",
    "net.add_to_nengo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let's extract out a desired answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D = 64\n",
    "subdim = 8\n",
    "N = 500\n",
    "\n",
    "import nef\n",
    "net=nef.Network('Symbols', fixed_seed=1, quick=True) #Create the network object\n",
    "\n",
    "net.make('A',1,D,mode='direct')\n",
    "net.make('B',1,D,mode='direct')\n",
    "net.make_array('C',N,D/subdim,dimensions=subdim,radius=1.0/math.sqrt(D), seed=2) \n",
    "\n",
    "conv = nef.convolution.make_convolution(net,'*','A','B','C',200, seed=3)\n",
    "\n",
    "\n",
    "net.make('E',1,D,mode='direct')\n",
    "net.make('F',1,D,mode='direct')\n",
    "\n",
    "conv = nef.convolution.make_convolution(net,'/','C','E','F',200, invert_second=True, seed=3)\n",
    "\n",
    "\n",
    "net.add_to_nengo()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But that's not much good without a memory\n",
    "- How do we add one?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D = 64\n",
    "subdim = 8\n",
    "N = 500\n",
    "\n",
    "import nef\n",
    "net=nef.Network('Symbols', fixed_seed=1, quick=True) #Create the network object\n",
    "\n",
    "net.make('A',1,D,mode='direct')\n",
    "net.make('B',1,D,mode='direct')\n",
    "net.make_array('C',N,D/subdim,dimensions=subdim,radius=1.0/math.sqrt(D), seed=2) \n",
    "\n",
    "net.connect('C', 'C', pstc=0.1)\n",
    "\n",
    "conv = nef.convolution.make_convolution(net,'*','A','B','C',200, seed=3)\n",
    "\n",
    "\n",
    "net.make('E',1,D,mode='direct')\n",
    "net.make('F',1,D,mode='direct')\n",
    "\n",
    "conv = nef.convolution.make_convolution(net,'/','C','E','F',200, invert_second=True, seed=3)\n",
    "\n",
    "\n",
    "net.add_to_nengo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Things to note\n",
    "    - Memory slowly decays\n",
    "    - If you push in a new pair for too long, it can wipe out the old pair(s)\n",
    "        - Note that this relies on the saturation behaviour of NEF networks\n",
    "        - Kind of like implicit normalization\n",
    "    - Memory capacity increases with dimensionality\n",
    "        - Also dependent on the number of different possible items in memory (vocabulary size)\n",
    "    - 512 dimensions is suffienct to store ~8 pairs, with a vocabulary size of 100,000 terms\n",
    "        - Note that this is what's needed for storing simple sentences"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
